{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# AIM329 - Build a chat assistant with Amazon Bedrock\n",
    "\n",
    "Welcome to session AIM329 of AWS re:Invent 2023 - Build a chat assistant with Amazon Bedrock.\n",
    "\n",
    "This notebook will walk you through the process of building a chat assistant using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/). We will use the [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) architecture with an Embeddings Model hosted on Amazon Bedrock to convert raw text to vectors and store and search them in an [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks.html\">Amazon SageMaker Studio Notebook</a> and in an AWS Region that supports <a href=\"https://aws.amazon.com/opensearch-service/features/serverless/\">Amazon OpenSearch Serverless</a>.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.m5.xlarge</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 3</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    1. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    2. [Install required software libraries](#Install%20required%20software%20libraries)\n",
    "    \n",
    "    3. [Configure logging](#Configure%20logging)\n",
    "        \n",
    "        1. [System logs](#Configure%20system%20logs)\n",
    "        \n",
    "        2. [Application logs](#Configure%20application%20logs)\n",
    "    \n",
    "    4. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    5. [Set AWS Region and boto3 config](#Set%20AWS%20Region%20and%20boto3%20config)\n",
    "    \n",
    "    6. [Check and create an Amazon OpenSearch Serverless collection](#Check%20and%20create%20an%20Amazon%20OpenSearch%20Serverless%20collection)\n",
    "    \n",
    "    7. [Enable model access in Amazon Bedrock](#Enable%20model%20access%20in%20Amazon%20Bedrock)\n",
    "    \n",
    "    8. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "\n",
    "    9. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    10. [Create an index in the Amazon OpenSearch Serverless collection](#Create%20index%20in%20collection)\n",
    "    \n",
    " 2. [Build the chat assistant](#Build%20the%20chat%20assistant)\n",
    "\n",
    "    1. [Architecture](#Architecture)\n",
    "    \n",
    "    2. [Step 0a: Prepare to load data into the vector database](#Step0a)\n",
    "        \n",
    "        1. [Initialize the text splitter](#Initialize%20the%20text%20splitter)\n",
    "        \n",
    "        2. [Prepare HTML files for loading](#Prepare%20HTML%20files%20for%20loading)\n",
    "        \n",
    "        3. [Prepare PDF files for loading](#Prepare%20PDF%20files%20for%20loading)\n",
    "    \n",
    "    3. [Step 0b and 0c: Create the embeddings](#Step0band0c)\n",
    "    \n",
    "    4. [Step 0d: Store the embeddings in the vector database](#Step0d)\n",
    "    \n",
    "    5. [Step 1 to 6: Build the chat steps](#Step1to6)\n",
    "    \n",
    " 3. [Chat with the assistant](#Chat%20with%20the%20assistant)\n",
    " \n",
    " 4. [Cleanup](#Cleanup)\n",
    " \n",
    " 5. [Conclusion](#Conclusion)\n",
    " \n",
    " 6. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f0b44",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> During the AIM329 session, by default, outbound Internet access will be enabled for this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Install required software libraries <a id ='Install%20required%20software%20libraries'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [LangChain](https://www.langchain.com/)\n",
    "* [Unstructured](https://pypi.org/project/unstructured/)\n",
    "* [OpenSearch Python Client](https://pypi.org/project/opensearch-py/)\n",
    "* [PyPDF2](https://pypi.org/project/PyPDF2/)\n",
    "* [AWS v4 authentication for the Python Requests library](https://pypi.org/project/requests-aws4auth/)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.29.4\n",
    "!pip install langchain==0.0.339\n",
    "!pip install unstructured==0.11.0\n",
    "!pip install opensearch-py==2.4.2\n",
    "!pip install PyPDF2==3.0.1\n",
    "!pip install requests-aws4auth==1.2.3\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {},
   "source": [
    "### C. Configure logging <a id ='Configure%20logging'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {},
   "source": [
    "####  a. System logs <a id='Configure%20system%20logs'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "Run the following cell to print the name of the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "import opensearchpy\n",
    "import PyPDF2\n",
    "import requests\n",
    "import sagemaker\n",
    "import sys\n",
    "from botocore.config import Config\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "logging.info(\"LangChain version : {}\".format(langchain.__version__))\n",
    "logging.info(\"OpenSearch Python Client version : {}\".format(opensearchpy.__version__))\n",
    "logging.info(\"PyPDF2 version : {}\".format(PyPDF2.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {},
   "source": [
    "###  E. Set AWS Region and boto3 config <a id ='Set%20AWS%20Region%20and%20boto3%20config'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. This will be used to initiate some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7632923",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock will use the current AWS Region. For Bedrock, follow the guidance in the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#bedrock-regions\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions. In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-west-2 (Oregon)</mark> for Amazon Bedrock.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3260b",
   "metadata": {},
   "source": [
    "###  F. Check and create an Amazon OpenSearch Serverless collection <a id ='Check%20and%20create%20an%20Amazon%20OpenSearch%20Serverless%20collection'> </a>\n",
    "\n",
    "This notebook uses an [Amazon OpenSearch Serverless (AOSS) collection](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-collections.html) of type [Vector search](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-usecase) as the vector database that will be used by the chat assistant.\n",
    "\n",
    "Run the following cells to check and create an AOSS collection if it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b06917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the flags to identify if an AOSS collection exists and if it is created through this notebook\n",
    "aoss_collection_exists = False\n",
    "aoss_collection_created = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2687632",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For the purpose of running this notebook, it is preferable to have an empty collection. During the AIM329 session, by default, an empty collection of type <i>Vector search</i> will be pre-created and ready to use.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcfd03",
   "metadata": {},
   "source": [
    "Run the following code cell to retreive the details of the first available AOSS collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4acac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the AOSS client\n",
    "aoss_client = boto3.client(\"opensearchserverless\", config = my_boto3_config)\n",
    "\n",
    "# Check and create a collection if none is found\n",
    "collection_id = ''\n",
    "collections = aoss_client.list_collections()['collectionSummaries']\n",
    "if len(collections) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"Found an AOSS collection.\")\n",
    "    first_collection = collections[0]\n",
    "    collection_id = first_collection[\"id\"]\n",
    "    collection_name = first_collection[\"name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31718d03",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you are running this notebook outside of the AIM329 session and would like to create an AOSS collection through this notebook, then, run the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846d5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: It may take 8 to 10 minutes to create the AOSS collection.\n",
    "\n",
    "# The helper function 'create_aoss_collection' (available through ./scripts/helper_functions.py) creates the specified\n",
    "# AOSS collection with the following policies:\n",
    "# Data access policy: provides full access to the IAM role associated with this notebook instance.\n",
    "# Encryption policy: encrypts with AWS owned key.\n",
    "# Network policy: provides public network access to the collection.\n",
    "\n",
    "if aoss_collection_exists:\n",
    "    logging.info(\"Skipping AOSS collection creation.\")\n",
    "else:\n",
    "    collection_name = \"aim329\"\n",
    "    data_access_policy_name = \"aim329-dap\"\n",
    "    encryption_policy_name = \"aim329-ep\"\n",
    "    network_policy_name = \"aim329-np\"\n",
    "    response = create_aoss_collection(aoss_client, collection_name, data_access_policy_name,\n",
    "                                      encryption_policy_name, network_policy_name, my_iam_role)\n",
    "    collection_id = response[\"id\"]\n",
    "    collection_name = response[\"name\"]\n",
    "    aoss_collection_created = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb9a28",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the AOSS collection that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7572f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(collection_id) == 0:\n",
    "    aoss_collection_exists = False\n",
    "    logging.info(\"No AOSS collections exist.\")\n",
    "else:\n",
    "    aoss_collection_exists = True\n",
    "    logging.info(\"The following AOSS collection will be used:\\nCollection id: {}; Collection name: {}\"\n",
    "                 .format(collection_id, collection_name))\n",
    "    # Print the AWS console URL to the AOSS collection\n",
    "    collection_aws_console_url = \"https://{}.console.aws.amazon.com/aos/home?region={}#opensearch/collections/{}\"\\\n",
    "    .format(my_region, my_region, collection_name)\n",
    "    logging.info(\"If you like to take a look at this collection, visit {}\".format(collection_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {},
   "source": [
    "###  G. Enable model access in Amazon Bedrock <a id ='Enable%20model%20access%20in%20Amazon%20Bedrock'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84ba9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> Before invoking any model in Amazon Bedrock, enable access to that model by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. In addition, for Anthropic models, you need to submit the use case details. Otherwise, you will get an authorization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de160b",
   "metadata": {},
   "source": [
    "Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Amazon Bedrock model access page URL\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you explicitly disable it, this is a one-time setup for each model in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  H. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell.\n",
    "\n",
    "This IAM role should have the following permissions,\n",
    "\n",
    "1. Full access to invoke Large Language Models (LLMs) on Amazon Bedrock.\n",
    "2. Full access to read and write to the Amazon OpenSearch Serverless collection created in the previous step.\n",
    "3. Access to write to Amazon CloudWatch Logs.\n",
    "\n",
    "In addition, [data access control](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-data-access.html) should be setup on the Amazon OpenSearch Serverless collection to provide create, read and write access to the IAM role associated with this notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f10781",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  During the AIM329 session, by default, all these permissions will be setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  I. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "To begin with, list all the available models in Amazon Bedrock by running the following cell. This will help you pick a LLM and the Embeddings model within Amazon Bedrock that you will be using in this notebook. By default, both will use the On-Demand pricing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e08e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List all the available foundation models in Amazon Bedrock\n",
    "models_info = ''\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "response = bedrock_client.list_foundation_models()\n",
    "model_summaries = response[\"modelSummaries\"]\n",
    "models_info = models_info + \"\\n\"\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(\"Provider Name\", \"Model Name\", \"Input Modalities\",\n",
    "                                                          \"Output Modalities\", \"Model Id\")\n",
    "models_info = models_info + \"-\".ljust(125, \"-\")\n",
    "for model_summary in model_summaries:\n",
    "    models_info = models_info + \"\\n\"\n",
    "    models_info = models_info + \"{:<15} {:<30} {:<20} {:<20} {:<40}\".format(model_summary[\"providerName\"],\n",
    "                                                                            model_summary[\"modelName\"],\n",
    "                                                                            \"|\".join(model_summary[\"inputModalities\"]),\n",
    "                                                                            \"|\".join(model_summary[\"outputModalities\"]),\n",
    "                                                                            model_summary[\"modelId\"])\n",
    "models_info = models_info + \"-\".ljust(125, \"-\") + \"\\n\"\n",
    "logging.info(\"Displaying available models in the '{}' Region:\".format(br_region) + models_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a835e",
   "metadata": {},
   "source": [
    "From the results of running the above cell,\n",
    "\n",
    "1. Pick the model-id that corresponds to the LLM that you want and set it as the value of the `llm_model_id` variable in the following cell.\n",
    "2. (Optional) Specify the [LLM-specific inference parameters](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html) in the `model_kwargs` parameter.\n",
    "2. Pick the model-id that corresponds to the Embeddings model that you want and set it as the value of the `embeddings_model_id` variable in the following cell.\n",
    "\n",
    "Now, run the following cell to create the common objects to be used in future steps in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e64ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> This notebook was tested with the following Amazon Bedrock models:\n",
    "    <li>LLMs: anthropic.claude-v2, anthropic.claude-instant-v1, cohere.command-text-v14, ai21.j2-ultra-v1</li>\n",
    "    <li>Embedding model(s): amazon.titan-embed-text-v1</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d6877",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> During the AIM329 session, it is recommended to ONLY use the following Amazon Bedrock models:\n",
    "    <li>LLMs: anthropic.claude-instant-v1</li>\n",
    "    <li>Embedding model(s): amazon.titan-embed-text-v1</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb830d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Specify the model-ids along with their inference parameters\n",
    "# Model-id of the LLM to be used in the chat assistant\n",
    "llm_model_id = \"anthropic.claude-instant-v1\"\n",
    "temperature = 0.5\n",
    "max_response_token_length = 300\n",
    "# Model-id of the Embeddings model to be used in the chat assistant\n",
    "embeddings_model_id = \"amazon.titan-embed-text-v1\"\n",
    "\n",
    "##### LLM related objects\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the LangChain client for the LLM using the Bedrock client created above.\n",
    "llm = Bedrock(\n",
    "    model_id = llm_model_id,\n",
    "    model_kwargs = get_model_specific_inference_params(llm_model_id,\n",
    "                                                       temperature,\n",
    "                                                       max_response_token_length),\n",
    "    client = bedrock_rt_client\n",
    ")\n",
    "\n",
    "##### Embeddings related objects\n",
    "# Use the LangChain BedrockEmbeddings class to create the Embeddings client.\n",
    "br_embeddings = BedrockEmbeddings(client = bedrock_rt_client, model_id = embeddings_model_id, region_name = br_region)\n",
    "\n",
    "##### Amazon OpenSearch Serverless (AOSS) related objects\n",
    "# Create the AOSS Python client from the AOSS boto3 client using the helper function \n",
    "# available through ./scripts/helper_functions.py)\n",
    "aoss_py_client = auth_opensearch(host = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "                            service = 'aoss', region = my_region)\n",
    "# Specify the name of the index in the AOSS collection; this will be created later in the notebook\n",
    "index_name = \"aim329-index\"\n",
    "# Specify the max workers for loading data in parallel into the index\n",
    "max_workers = 8\n",
    "# To access an Opensearch Collection using LangChain, we can use the OpenSearchVectorSearch class.\n",
    "doc_search = OpenSearchVectorSearch(\n",
    "    opensearch_url = \"{}.{}.aoss.amazonaws.com\".format(collection_id, my_region),\n",
    "    index_name = index_name,\n",
    "    embedding_function = br_embeddings)\n",
    "# Set the doc search client to the AOSS Python client\n",
    "doc_search.client = aoss_py_client\n",
    "\n",
    "##### File related objects\n",
    "# Specify the path to the directory that will contain the RAG data\n",
    "rag_dir = os.path.join(os.getcwd(), \"data/rag\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(rag_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4b73",
   "metadata": {},
   "source": [
    "###  J. Create an index in the Amazon OpenSearch Serverless collection <a id='Create%20index%20in%20collection'></a>\n",
    "\n",
    "To create an index in the Amazon OpenSearch Serverless (AOSS) collection, we first need to define a schema for our index. AOSS allows users to specify a simple search index, which utilizes keyword matching, or the vector search feature, which utilizes [k-Nearest Neighbor (k-NN) search](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/knn.html). Vector search differs from standard search in that instead of using a typical keyword matching or fuzzy matching algorithm, vector search compares [embeddings](https://en.wikipedia.org/wiki/Word_embedding) of two pieces of text. An embedding is a numerical representation of a piece of information, like text, that we can compare against other embeddings. To learn more about embeddings, take a look at [this blog](https://huggingface.co/blog/getting-started-with-embeddings). The vector search feature allows us to search for documents that are semantically similar to the questions that our end users send to our chat assistant. This can improve the context that we then give to our LLM to answer the user's questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ff129-c6f3-484e-bf8c-15269e10f822",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the schema for the index with an k-NN type vector as the embedding\n",
    "knn_index = {\n",
    "    \"settings\": {\n",
    "        \"index.knn\": True,\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "           \"content-embedding\": { \n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1536 # can have dimension up to 10k\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"source\": {\n",
    "                \"type\": \"text\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Delete the index\n",
    "#aoss_py_client.indices.delete(index = index_name)\n",
    "\n",
    "# Create the index if it does not exist\n",
    "if aoss_py_client.indices.exists(index = index_name):\n",
    "    logging.info(\"AOSS index '{}' already exists.\".format(index_name))\n",
    "else:\n",
    "    logging.info(\"Creating AOSS index '{}'...\".format(index_name))\n",
    "    logging.info(aoss_py_client.indices.create(index = index_name, body = knn_index, ignore = 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e2d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the AWS console URL to the AOSS index\n",
    "index_aws_console_url = collection_aws_console_url + \"/\" + index_name\n",
    "logging.info(\"If you like to take a look at this index, visit {}\".format(index_aws_console_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5d414",
   "metadata": {},
   "source": [
    "## 2. Build the chat assistant <a id ='Build%20the%20chat%20assistant'> </a>\n",
    "\n",
    "Large language models (LLMs) have a tendency to [hallucinate](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)). Hallucination in a LLM context is our model providing a confident but factually incorrect response that often tells us what the model thinks we want to hear, regardless of if it actually is the correct answer. One way to prevent LLMs from giving us incorrect information is by using a [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) mechanism.\n",
    "\n",
    "RAG allows us to provide our model with correct context information that it can use to ground its response in facts, instead of it trying to remember facts from its training data. To setup RAG, we need to have a document database that we can utilize to provide our model with related source documents. There are many ways to setup a document database. In this notebook, we will use an [Amazon Opensearch Serverless](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html) collection of type 'Vector search'.\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/) to orchestrate the sequence of events performed by the chat assistant. LangChain is a framework designed to simplify the creation of LLM applications. It provides flexible abstractions and an extensive toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eb789",
   "metadata": {},
   "source": [
    "###  A. Architecture <a id='Architecture'></a>\n",
    "\n",
    "![Architecture](./images/architecture.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aac8ba",
   "metadata": {},
   "source": [
    "###  B. Step 0a: Prepare to load data into the vector database <a id='Step0a'></a>\n",
    "\n",
    "An Amazon OpenSearch Serverless (AOSS) collection is a logical grouping of one or more indexes that work together to support a specific workload or use case.\n",
    "\n",
    "This notebook will use a vector index for indexing documents in the AOSS collection. To demonstrate the versatility of the data that can be processed, we will ingest data from some HTML and PDF files. During the ingestion process, we will extract text from these files and split them into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679ed1c8",
   "metadata": {},
   "source": [
    "####  a. Initialize the text splitter <a id='Initialize%20the%20text%20splitter'></a>\n",
    "\n",
    "When we are indexing documents for information retrieval, providing an entire document to a LLM as context can be overwhelming to our LLM, especially for very long documents. A best practice is to divide the document into easier to consume partially overlapping chunks. Dividing the document in this way also tends to improve search result relevance as often the answer we are looking for is contained within a specific passage of a document and providing the entire document is unnecessary. \n",
    "\n",
    "Let's use the LangChain's [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) to create a text splitting object that we will use split the content before loading into the vector database. Here, we will use the simple `fixed-size chunking` strategy where we will set the size of each chunk and the number of overlapping characters between two consecutive chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4187eb0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 8000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54173f4b",
   "metadata": {},
   "source": [
    "####  b. Prepare HTML files for loading <a id='Prepare%20HTML%20files%20for%20loading'></a>\n",
    "\n",
    "Run the following cells to populate some documentation on Amazon Bedrock so our chat assistant can answer questions about Amazon Bedrock with factually correct information.\n",
    "\n",
    "These documentation are available as HTML files. As a first step, download these files to the local directory named `./data/rag/amazon-bedrock-docs/`. The required directory structure will be created if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e2feb9-af2d-4e95-8db0-23a5ef4e88b7",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple list of Agents for Amazon Bedrock documentation to index\n",
    "html_link_list = [\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-service.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/setting-up.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/embeddings.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-prepare.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-guidelines.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/prov-throughput.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html\",\n",
    "    \"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html\"\n",
    "]\n",
    "\n",
    "# Set the sub-directory to store these HTML files\n",
    "html_files_dir = \"{}/{}\".format(rag_dir, \"amazon-bedrock-docs\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(html_files_dir, exist_ok = True)\n",
    "\n",
    "# Download and store each HTML file; overwrite existing file\n",
    "for html_link in html_link_list:\n",
    "    html_content = requests.get(html_link).content.decode('utf-8')\n",
    "    file_name = html_link.split(\"/\")[-1]\n",
    "    with open('{}/{}'.format(html_files_dir, file_name), \"w\") as f:\n",
    "        f.write(html_content)\n",
    "        logging.info(\"Downloaded file '{}' to '{}'\".format(file_name, html_files_dir))\n",
    "\n",
    "logging.info(\"A total of {} HTML files were downloaded to '{}'.\".format(len(html_link_list), html_files_dir))\n",
    "        \n",
    "# Display the last downloaded HTML file\n",
    "#HTML(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157d8a0",
   "metadata": {},
   "source": [
    "Now we have downloaded all of these HTML documents, let's go ahead and load them using a HTML loader. We are going to use LangChain's [Unstructured HTML Loader](https://python.langchain.com/docs/modules/data_connection/document_loaders/html). This will parse the raw HTML files and put it in a format that we can then give to our LLMs. Finally we will split each document as per the splitter configuration defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7537003-aebd-4d1a-911c-67e4d62c3f7c",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# Initialize\n",
    "html_doc_data_list = []\n",
    "html_doc_link_list = []\n",
    "\n",
    "# Loop through the downloaded HTML files in the directory\n",
    "for html_link in html_link_list:\n",
    "    file_name = html_link.split(\"/\")[-1]\n",
    "    file_path = '{}/{}'.format(html_files_dir, file_name)\n",
    "    \n",
    "    # Load the file content\n",
    "    loader = UnstructuredHTMLLoader(file_path)\n",
    "    data = loader.load()\n",
    "    \n",
    "    # Remove irrelevant text\n",
    "    html_doc = data[0].page_content.replace(\"\"\"Did this page help you? - Yes\n",
    "\n",
    "Thanks for letting us know we're doing a good job!\n",
    "\n",
    "If you've got a moment, please tell us what we did right so we can do more of it.\n",
    "\n",
    "Did this page help you? - No\n",
    "\n",
    "Thanks for letting us know this page needs work. We're sorry we let you down.\n",
    "\n",
    "If you've got a moment, please tell us how we can make the documentation better.\"\"\", \"\").replace(\"\"\"Javascript is disabled or is unavailable in your browser.\n",
    "\n",
    "To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser's Help pages for instructions.\"\"\", \"\")\n",
    "    \n",
    "    # Split our document into chunks\n",
    "    texts = text_splitter.create_documents([html_doc])\n",
    "    \n",
    "    # Create a list of document chunks as well as a list of links\n",
    "    for text in texts:\n",
    "        html_doc_data_list.append(text.page_content)\n",
    "        html_doc_link_list.append(html_link)\n",
    "\n",
    "logging.info(\"Created {} chunks from the {} downloaded HTML files.\".format(len(html_doc_data_list), len(html_link_list)))        \n",
    "\n",
    "# Print the first chunk\n",
    "#logging.info(html_doc_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3297178c",
   "metadata": {},
   "source": [
    "####  c. Prepare PDF files for loading <a id='Prepare%20PDF%20files%20for%20loading'></a>\n",
    "\n",
    "Run the following cells to populate some Generative AI related whitepapers so our chat assistant can answer questions about them with factually correct information.\n",
    "\n",
    "These whitepapers are available as PDF files. As a first step, download these files to the local directory named `./data/rag/gen-ai-whitepapers/`. The required directory structure will be created if it doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86077e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple list of Gen AI whitepapers to index\n",
    "pdf_link_list = [\n",
    "    \"https://www-cdn.anthropic.com/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226/Model-Card-Claude-2.pdf\"\n",
    "]\n",
    "\n",
    "# Set the sub-directory to store these PDF files\n",
    "pdf_files_dir = \"{}/{}\".format(rag_dir, \"gen-ai-whitepapers\")\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(pdf_files_dir, exist_ok = True)\n",
    "\n",
    "# Download and store each PDF file; overwrite existing file\n",
    "for pdf_link in pdf_link_list:\n",
    "    pdf_content = requests.get(pdf_link).content\n",
    "    file_name = pdf_link.split(\"/\")[-1]\n",
    "    with open('{}/{}'.format(pdf_files_dir, file_name), \"wb\") as f:\n",
    "        f.write(pdf_content)\n",
    "        logging.info(\"Downloaded file '{}' to '{}'\".format(file_name, pdf_files_dir))\n",
    "\n",
    "logging.info(\"A total of {} PDF files were downloaded to '{}'.\".format(len(pdf_link_list), pdf_files_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f32a7",
   "metadata": {},
   "source": [
    "Now we have downloaded all of these PDF documents, let's go ahead and process them. We are going to extract the text from them using the [PyPDF2](https://pypdf2.readthedocs.io/en/3.0.0/) library. Finally we will split each document as per the splitter configuration defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c9ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Initialize\n",
    "pdf_doc_data_list = []\n",
    "pdf_doc_link_list = []\n",
    "\n",
    "# Loop through the downloaded PDF files in the directory\n",
    "for pdf_link in pdf_link_list:\n",
    "    pdf_doc = ''\n",
    "    file_name = pdf_link.split(\"/\")[-1]\n",
    "    file_path = '{}/{}'.format(pdf_files_dir, file_name)\n",
    "    \n",
    "    # Load the file content\n",
    "    reader = PdfReader(file_path)\n",
    "    \n",
    "    # Loop through the pages\n",
    "    for page in reader.pages:\n",
    "        pdf_doc = pdf_doc + page.extract_text()\n",
    "    \n",
    "    # Split our document into chunks\n",
    "    texts = text_splitter.create_documents([pdf_doc])\n",
    "    \n",
    "    # Create a list of document chunks as well as a list of links\n",
    "    for text in texts:\n",
    "        pdf_doc_data_list.append(text.page_content)\n",
    "        pdf_doc_link_list.append(pdf_link)\n",
    "\n",
    "logging.info(\"Created {} chunks from the {} downloaded PDF files.\".format(len(pdf_doc_data_list), len(pdf_link_list)))        \n",
    "\n",
    "# Print the first chunk\n",
    "#logging.info(pdf_doc_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12b335-5d79-45a7-b4d2-b487fefd6f64",
   "metadata": {},
   "source": [
    "###  C. Step 0b and 0c: Create the embeddings <a id='Step0band0c'></a>\n",
    "\n",
    "Now that our document chunks ready, let us vectorize them to create the embeddings. Along with this, we will prepare the documents to be inserted into the AOSS collection's index.\n",
    "\n",
    "Each of those prepared documents will contain the following fields,\n",
    "- `content` - contains the actual text of the document chunk.\n",
    "- `content-embedding` - contains the corresponding document chunk's embedding.\n",
    "- `title` - a reference to the name of the location from where the content was ingested.\n",
    "- `source` - a reference to the location from where the content was ingested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e37392-7803-4c64-b074-3c525b6cb840",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the embeddings from the document chunks and prepare documents to index\n",
    "# by using the 'prepare_index_document_list' helper function available through ./scripts/helper_functions.py)\n",
    "\n",
    "# Process the chunked HTML documents\n",
    "html_doc_list = prepare_index_document_list(br_embeddings, 'HTML', html_doc_data_list, html_doc_link_list)\n",
    "\n",
    "# Process the chunked PDF documents\n",
    "pdf_doc_list = prepare_index_document_list(br_embeddings, 'PDF', pdf_doc_data_list, pdf_doc_link_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1801276",
   "metadata": {},
   "source": [
    "###  D. Step 0d: Store the embeddings in the vector database <a id='Step0d'></a>\n",
    "\n",
    "Run the following cell to upload the prepared documents into our created AOSS collection's index. The below function uses a parallel processing function to upload our documents into our index. The number of parallel worker threads is controlled by the `max_workers` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0d756",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> After executing the below cell, it may take up to 30 seconds for the data to be available for reading. If there are any boto3 errors, then the API calls will be retried automatically based on the settings in the earlier step.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501ad3c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the time of writing this notebook, AOSS did not support ingestion with <i>id</i> for <i>Vector search</i> collection type. As a result, running the following cell more than once will result in duplicate documents being created in the AOSS index. This is ok for the purpose of running this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d765149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the function to import into the AOSS index.\n",
    "def os_import(article):\n",
    "    \"\"\"\n",
    "    This function imports the documents and their metadata into the AOSS index.\n",
    "    \"\"\"\n",
    "    aoss_py_client.index(index = index_name,\n",
    "                         body={\n",
    "                                \"content-embedding\": article['content-embedding'],\n",
    "                                \"content\": article['content'],\n",
    "                                \"title\": article['title'],\n",
    "                                \"source\": article['source'],\n",
    "                              }\n",
    "                        )\n",
    "    \n",
    "# Parallelize and populate the AOSS Collection's index with HTML data\n",
    "process_map(os_import, html_doc_list, max_workers = max_workers)\n",
    "\n",
    "# Parallelize and populate the AOSS Collection's index with PDF data\n",
    "process_map(os_import, pdf_doc_list, max_workers = max_workers) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa1860",
   "metadata": {},
   "source": [
    "###  E. Step 1 to 6: Build the chat steps <a id='Step1to6'></a>\n",
    "\n",
    "In order to demonstrate the usefulness of the [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) architecture, let us directly call the LLM without any searches on the vector database. Here, we will ask a question about Agents for Amazon Bedrock or about Anthropic Claude models' performance which we know the LLM will not be aware of because these were not available when the LLM was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2af9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the query\n",
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "query_char_count, query_word_count = get_counts_from_text(query)\n",
    "# Invoke the LLM\n",
    "output = llm.generate([query])\n",
    "# Read the response\n",
    "query_response = output.generations[0][0].text\n",
    "query_resp_char_count, query_resp_word_count = get_counts_from_text(query_response)\n",
    "\n",
    "# Print the details\n",
    "logging.info(\"\\n\\nHuman:\\n{}\\n\\nAssistant:\\n{}\\n\".format(query, query_response))\n",
    "\n",
    "# Print the stats\n",
    "logging.info(\"Query (prompt) stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_char_count, query_word_count))\n",
    "logging.info(\"Response stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_resp_char_count, query_resp_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd2117",
   "metadata": {},
   "source": [
    "While this response may seem plausible, it is actually incorrect. In general, LLMs will try to answer your question but in this case it has hallucinated. This is an example of how the LLM was not able to provide the correct answer. Here is where a RAG architecture will provide the remedy.\n",
    "\n",
    "Let us see if we can find a document that contains information about Agents for Amazon Bedrock or about Anthropic Claude models' performance in our document index. We can do this in two ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e32b5",
   "metadata": {},
   "source": [
    "Method 1: Using the AOSS Python client's `search` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3043bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "temp_embedding = br_embeddings.embed_query(text = query)\n",
    "search_query = {\"query\": {\"knn\": {\"content-embedding\": {\"vector\": temp_embedding, \"k\": 5}}}}\n",
    "results = aoss_py_client.search(index = index_name, body = search_query)\n",
    "hits = results[\"hits\"][\"hits\"]\n",
    "logging.info(\"Found {} hit(s).\".format(len(hits)))\n",
    "for hit in hits:\n",
    "    logging.info(hit[\"_source\"][\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32757e9e",
   "metadata": {},
   "source": [
    "Method 2: Using LangChain's `similarity_search` which will use the AOSS Python client under the covers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f6564",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_results = 5\n",
    "query = \"What are Agents in Amazon Bedrock?\"\n",
    "#query = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "docs = doc_search.similarity_search(\n",
    "    # Our text query\n",
    "    query = query,\n",
    "    # The name of the field that contains our vector\n",
    "    vector_field = \"content-embedding\",\n",
    "    # The actual text field we are looking for\n",
    "    text_field = \"content\",\n",
    "    # The number of results we want to return\n",
    "    k = max_results\n",
    ")\n",
    "logging.info(\"Specified {} max results. Found {} hit(s).\".format(max_results, len(docs)))\n",
    "for doc in docs:\n",
    "    logging.info(docs[0].metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887ea8ad",
   "metadata": {},
   "source": [
    "It looks like we do have some information on Agents for Amazon Bedrock or the RefinedWeb Dataset for Falcom LLM in the documents that we prepared and stored in the AOSS collection's index. Now that we know we have the right information in our document index, let us setup a [RetrievalQA chain](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa). This chain allows us to supply a [prompt template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), our LLM, and our document index to form a question answering chain that will answer questions based on the returned context document. We will use the [stuff](https://python.langchain.com/docs/modules/chains/document/stuff) chain type.\n",
    "\n",
    "[Prompt templates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/) are pre-defined recipes for generating prompts for language models. In the one we create below, we specify context and question input variables, which our RetrievalQA chain will fill in with the query and source documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad37629",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the prompt template\n",
    "prompt_template = \"\"\"\\n\\nHuman: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Don't include harmful content.\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\\n\\nAssistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template = prompt_template, input_variables = [\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the Retrieval QA chain\n",
    "qa = RetrievalQA.from_chain_type(llm = llm, \n",
    "                                 chain_type = \"stuff\", \n",
    "                                 retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                     \"vector_field\": \"content-embedding\",\n",
    "                                     \"text_field\": \"content\",\n",
    "                                     \"k\": 5}),\n",
    "                                 return_source_documents = True,\n",
    "                                 chain_type_kwargs = {\"prompt\": PROMPT, \"verbose\": False},\n",
    "                                 verbose = False)\n",
    "\n",
    "# Ask the question to the LLM and print the response along with the references from the source\n",
    "question = \"What are Agents in Amazon Bedrock?\"\n",
    "#question = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "# Invoke the embedding search, LLM etc. through the LangChain RetrievalQA chain\n",
    "response = qa(question, return_only_outputs = True)\n",
    "# Parse the output\n",
    "question, answer, context, title, source = parse_rqa_prompt_output(question, response)\n",
    "\n",
    "# Print the details\n",
    "prompt_template_with_result = prompt_template + answer + \"\\n\"\n",
    "logging.info(prompt_template_with_result.format(context=context, question=question))\n",
    "logging.info(\"The context for the above question was retreived from here --> Title: {}; Source: {}\"\n",
    "             .format(title, source))\n",
    "\n",
    "# Print the stats\n",
    "query_char_count, query_word_count = get_counts_from_text(question)\n",
    "logging.info(\"Question stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_char_count, query_word_count))\n",
    "query_resp_char_count, query_resp_word_count = get_counts_from_text(answer)\n",
    "logging.info(\"Answer stats: Character count = {}; Word count = {}\"\n",
    "             .format(query_resp_char_count, query_resp_word_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9202e239-b6e8-4252-9f8d-b81c499478a9",
   "metadata": {},
   "source": [
    "Let's now take it one step further with a [ConversationalRetrievalChain](https://python.langchain.com/docs/expression_language/cookbook/retrieval#conversational-retrieval-chain). \n",
    "\n",
    "LLMs on their own will not remember the last input you provided them. So we need a mechanism to remember and supply our previous conversation information back to our LLM. We can do this by pairing with a [ConversationBufferMemory](https://python.langchain.com/docs/modules/memory/adding_memory) class. This will allow us to hold a conversation with our LLM and retain the previous conversation in memory.\n",
    "\n",
    "The following cells add a conversational element to the retrieval chain and allows us to add chat memory to the retrieval. This chain uses a LLM call prior to the document retrieval that condenses conversation history and the current question into a single new question to improve document retrieval.\n",
    "\n",
    "To begin with, construct the prompt template and instantiate the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e71e061-4ea4-45a5-ac76-c78945942681",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the Conversational Retrieval chain\n",
    "cqa = ConversationalRetrievalChain.from_llm(llm = llm, \n",
    "                                            chain_type = \"stuff\", \n",
    "                                            condense_question_llm = llm,\n",
    "                                            retriever = doc_search.as_retriever(search_kwargs = {\n",
    "                                                \"vector_field\": \"content-embedding\",\n",
    "                                                \"text_field\": \"content\",\n",
    "                                                \"k\": 5}),\n",
    "                                            return_source_documents = True,\n",
    "                                            memory = ConversationBufferMemory(input_key = \"question\",\n",
    "                                                                              output_key = \"answer\",\n",
    "                                                                              memory_key = \"chat_history\",\n",
    "                                                                              return_messages = True),\n",
    "                                            verbose = False)\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97605629",
   "metadata": {},
   "source": [
    "Ask the first question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77842717",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How did Claude 2 do on the Graduate Record Exam?\"\n",
    "# Invoke the embedding search, LLM etc. through the LangChain ConversationalRetrievalChain chain\n",
    "response = cqa.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "# Parse and print the output\n",
    "logging.info(convert_crc_chat_history_to_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a28139",
   "metadata": {},
   "source": [
    "Ask the second question. Now when the response is printed, the previous message exchange will also be printed as it is stored in the chat history in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are Agents in Amazon Bedrock?\"\n",
    "# Invoke the embedding search, LLM etc. through the LangChain ConversationalRetrievalChain chain\n",
    "response = cqa.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "# Parse and print the output\n",
    "logging.info(convert_crc_chat_history_to_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d6d33-e76c-483c-9fc8-f84f3e81e273",
   "metadata": {},
   "source": [
    "## 3. Chat with the assistant <a id='Chat%20with%20the%20assistant'></a>\n",
    "\n",
    "Now we are going to put it all together in a single browser interface. The below call will initiate a simple conversational UI inside of our notebook. Run it and start asking questions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b148c66-6bce-4fb4-a784-b51907912cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The class 'ChatUX' is defined in ./scripts/helper_functions.py\n",
    "# Instantiate it and start the interactive chat with the assistant\n",
    "\n",
    "chatux = ChatUX(cqa)\n",
    "chatux.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1216a",
   "metadata": {},
   "source": [
    "## 4. Cleanup <a id='Cleanup'></a>\n",
    "\n",
    "As a best practice, you should delete AWS resources that are no longer required.  This will help you avoid incurring unncessary costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34a28a3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> During the AIM329 session, by default, all resources will be cleaned up at the end of the session. If you are running this notebook outside of the AIM329 session, you can cleanup the AOSS resources created through this notebook by running the following cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c3f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The helper function 'delete_aoss_collection' (available through ./scripts/helper_functions.py) deletes the specified\n",
    "# AOSS collection along with all the indexes in it. In addition, it also deletes the specified data access policy,\n",
    "# encryption policy and network policy.\n",
    "\n",
    "if aoss_collection_created:\n",
    "    delete_aoss_collection(aoss_client, collection_id, data_access_policy_name,\n",
    "                           encryption_policy_name, network_policy_name)\n",
    "else:\n",
    "    logging.info(\"Skipping AOSS collection deletion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to build chat assistant using a Large Language Model (LLM) hosted on Amazon Bedrock. In the process, we also demonstrated how a Retrieval Augment Generation (RAG) mechanism can help prevent hallucination. While using RAG, we showed you how to use an Embeddings Model hosted on Amazon Bedrock to convert raw text to vectors and how to store and search them in an Amazon OpenSearch Serverless collection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 6. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, Amazon OpenSearch Serverless, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: What is the difference between OpenSearch, Amazon OpenSearch Serverless, and Amazon OpenSearch Service?**\n",
    "\n",
    "OpenSearch is a fully open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, and clickstream analysis. For more information, see the [OpenSearch documentation](https://opensearch.org/docs/latest/).\n",
    "\n",
    "Amazon OpenSearch Service provisions all the resources for your OpenSearch cluster and launches it. It also automatically detects and replaces failed OpenSearch Service nodes, reducing the overhead associated with self-managed infrastructures. You can scale your cluster with a single API call or a few clicks in the console.\n",
    "\n",
    "Amazon OpenSearch Serverless is an on-demand serverless configuration for Amazon OpenSearch Service. Serverless removes the operational complexities of provisioning, configuring, and tuning your OpenSearch clusters. It's a good option for organizations that don't want to self-manage their OpenSearch clusters, or organizations that don't have the dedicated resources or expertise to operate large clusters. With OpenSearch Serverless, you can easily search and analyze a large volume of data without having to worry about the underlying infrastructure and data management.\n",
    "\n",
    "**Q: How does Amazon OpenSearch Serverless manage capacity?**\n",
    "\n",
    "With Amazon OpenSearch Serverless, you don't have to manage capacity yourself. OpenSearch Serverless automatically scales compute capacity for your account based on the current workload. Serverless compute capacity is measured in OpenSearch Compute Units (OCUs). Each OCU is a combination of 6 GiB of memory and corresponding virtual CPU (vCPU), as well as data transfer to Amazon S3. For more information about the decoupled architecture in OpenSearch Serverless, see [How it works](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless-overview.html#serverless-process).\n",
    "\n",
    "**Q: Will Amazon Bedrock capture and store my data?**\n",
    "\n",
    "Amazon Bedrock doesn't use your prompts and continuations to train any AWS models or distribute them to third parties. Your training data isn't used to train the base Amazon Titan models or distributed to third parties. Other usage data, such as usage timestamps, logged account IDs, and other information logged by the service, is also not used to train the models.\n",
    "\n",
    "Amazon Bedrock uses the fine tuning data you provide only for fine tuning an Amazon Titan model. Amazon Bedrock doesn't use fine tuning data for any other purpose, such as training base foundation models.\n",
    "\n",
    "Each model provider has an escrow account that they upload their models to. The Amazon Bedrock inference account has permissions to call these models, but the escrow accounts themselves don't have outbound permissions to Amazon Bedrock accounts. Additionally, model providers don't have access to Amazon Bedrock logs or access to customer prompts and continuations.\n",
    "\n",
    "Amazon Bedrock doesn’t store or log your data in its service logs.\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html#models-supported).\n",
    "\n",
    "**Q: What is the difference between On-demand and Provisioned Throughput in Amazon Bedrock?**\n",
    "\n",
    "With the On-Demand mode, you only pay for what you use, with no time-based term commitments. For text generation models, you are charged for every input token processed and every output token generated. For embeddings models, you are charged for every input token processed. A token is comprised of a few characters and refers to the basic unit that a model learns to understand user input and prompt to generate results. For image generation models, you are charged for every image generated.\n",
    "\n",
    "With the Provisioned Throughput mode, you can purchase model units for a specific base or custom model. The Provisioned Throughput mode is primarily designed for large consistent inference workloads that need guaranteed throughput. Custom models can only be accessed using Provisioned Throughput. A model unit provides a certain throughput, which is measured by the maximum number of input or output tokens processed per minute. With this Provisioned Throughput pricing, charged by the hour, you have the flexibility to choose between 1-month or 6-month commitment terms.\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find resources for prompt engineering?**\n",
    "\n",
    "[Prompt Engineering Guide](https://www.promptingguide.ai/).\n",
    "\n",
    "**Q: Is LangChain mandatory to use Amazon Bedrock?**\n",
    "\n",
    "No. You can interact with Amazon Bedrock using the [Bedrock API](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) or language-specific [AWS SDKs](https://aws.amazon.com/developer/tools/). Using LangChain will simplify the orchestration of the steps involved in the interactions between various components involved in this architecture. \n",
    "\n",
    "**Q: How do I get started with LangChain?**\n",
    "\n",
    "Go [here](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon OpenSearch Serverless pricing - go [here](https://aws.amazon.com/opensearch-service/pricing/) and navigate to the <i>Serverless</i> section.\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc1023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
